{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5de296-05cd-40a3-b77d-5cf70e056fe5",
   "metadata": {},
   "source": [
    "## VC-SD: Demonstration\n",
    "\n",
    "This script demonstrate the voice conversion, voice design and controllability of the VC-SD framework. Please note, this script is for demonstration purposes only, final models, training schemes, practical implementation etc. is not provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba5ab3-a344-4ffd-8447-9615435083e5",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "457c1e3d-b5a8-4f40-9349-60498da3eab4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/jupyter-arbu/.local/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: librosa in /home/jupyter-arbu/.local/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in /home/jupyter-arbu/.local/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: ipywidgets in /opt/tljh/user/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: descript-audiotools in /home/jupyter-arbu/.local/lib/python3.12/site-packages (0.7.4)\n",
      "Requirement already satisfied: torchfcpe in /home/jupyter-arbu/.local/lib/python3.12/site-packages (0.0.4)\n",
      "Requirement already satisfied: filelock in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.12/site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/tljh/user/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/tljh/user/lib/python3.12/site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/tljh/user/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/tljh/user/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/tljh/user/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: argbind in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.3.9)\n",
      "Requirement already satisfied: pyloudnorm in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.1.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/tljh/user/lib/python3.12/site-packages (from descript-audiotools) (6.5.2)\n",
      "Requirement already satisfied: julius in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.2.7)\n",
      "Requirement already satisfied: torchaudio in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (2.5.1)\n",
      "Requirement already satisfied: ffmpy in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.5.0)\n",
      "Requirement already satisfied: rich in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (14.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (3.10.1)\n",
      "Requirement already satisfied: pystoi in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.4.1)\n",
      "Requirement already satisfied: torch-stoi in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.2.3)\n",
      "Requirement already satisfied: flatten-dict in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.4.2)\n",
      "Requirement already satisfied: markdown2 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (2.5.3)\n",
      "Requirement already satisfied: randomname in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (0.2.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (4.25.7)\n",
      "Requirement already satisfied: tensorboard in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (2.19.0)\n",
      "Requirement already satisfied: tqdm in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from descript-audiotools) (4.67.1)\n",
      "Requirement already satisfied: einops in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torchfcpe) (0.8.1)\n",
      "Requirement already satisfied: local-attention in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from torchfcpe) (1.11.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/tljh/user/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/tljh/user/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/tljh/user/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/tljh/user/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/tljh/user/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/tljh/user/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/tljh/user/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.12/site-packages (from lazy_loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/tljh/user/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/tljh/user/lib/python3.12/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/tljh/user/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pyyaml in /opt/tljh/user/lib/python3.12/site-packages (from argbind->descript-audiotools) (6.0.2)\n",
      "Requirement already satisfied: docstring-parser in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from argbind->descript-audiotools) (0.16)\n",
      "Requirement already satisfied: six<2.0,>=1.12 in /opt/tljh/user/lib/python3.12/site-packages (from flatten-dict->descript-audiotools) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: hyper-connections>=0.1.8 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from local-attention->torchfcpe) (0.1.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from matplotlib->descript-audiotools) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from matplotlib->descript-audiotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from matplotlib->descript-audiotools) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from matplotlib->descript-audiotools) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from matplotlib->descript-audiotools) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from matplotlib->descript-audiotools) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/tljh/user/lib/python3.12/site-packages (from matplotlib->descript-audiotools) (2.9.0.post0)\n",
      "Requirement already satisfied: future>=0.16.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from pyloudnorm->descript-audiotools) (1.0.0)\n",
      "Requirement already satisfied: fire in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from randomname->descript-audiotools) (0.7.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from rich->descript-audiotools) (3.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from tensorboard->descript-audiotools) (2.2.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from tensorboard->descript-audiotools) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from tensorboard->descript-audiotools) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from tensorboard->descript-audiotools) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from tensorboard->descript-audiotools) (3.1.3)\n",
      "Requirement already satisfied: pycparser in /opt/tljh/user/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/tljh/user/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->descript-audiotools) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/tljh/user/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/tljh/user/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: termcolor in /home/jupyter-arbu/.local/lib/python3.12/site-packages (from fire->randomname->descript-audiotools) (3.1.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/tljh/user/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/tljh/user/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/tljh/user/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install torch librosa numpy ipywidgets descript-audiotools torchfcpe\n",
    "print(\"All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16fd45-900d-4571-bdcf-58a9071d2b6c",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ae3c903-778b-4fb6-9fde-eeb3636d4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from utils.demo_utils import *\n",
    "from audiotools import transforms as tfm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be75ac0-fa16-408a-ba5d-f46919b77a7f",
   "metadata": {},
   "source": [
    "## Voice Design\n",
    "\n",
    "Use the sliders to create a new voice profile for the input audio. \\\n",
    "The input audio file can be switched by changing the \"audio file\" path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3317ac5a-6ce8-4bb8-982e-5ab490a8a13b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2b8947d7414749a19b8b44b8845810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='audio/librispeech2.wav', continuous_update=False, description='Audio File:', placeh…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def linear_map(x, src_min, src_max, dst_min, dst_max):\n",
    "    x = np.clip(x, src_min, src_max)\n",
    "    return dst_min + (x - src_min) * (dst_max - dst_min) / (src_max - src_min)\n",
    "\n",
    "def years_to_age_param(years):\n",
    "    \"\"\"Convert years back to age parameter\"\"\"\n",
    "    return linear_map(\n",
    "        years,\n",
    "        src_min=15,\n",
    "        src_max=90,\n",
    "        dst_min=-0.75,\n",
    "        dst_max=3.5,\n",
    "    )\n",
    "\n",
    "def semitones_to_pitch(semitones):\n",
    "    \"\"\"Convert semitones to pitch multiplier (octaves)\"\"\"\n",
    "    return 2 ** (semitones / 12.0)\n",
    "\n",
    "def gender_param_to_label(gender_param):\n",
    "    \"\"\"Convert gender parameter to label\"\"\"\n",
    "    if gender_param < 0:\n",
    "        return \"Male\"\n",
    "    else:\n",
    "        return \"Female\"\n",
    "\n",
    "transform = tfm.Compose(\n",
    "            tfm.VolumeNorm(),\n",
    "            tfm.RescaleAudio())\n",
    "vc_model = torch.jit.load(\"pretrained/model-nc.ts\")\n",
    "vc_model = vc_model.eval()\n",
    "\n",
    "audio_path = widgets.Text(\n",
    "    value='audio/librispeech2.wav',\n",
    "    placeholder='Enter audio file path',\n",
    "    description='Audio File:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "gender_slider = widgets.FloatSlider(value=-0.1, min=-1.72, max=1.94, step=0.01, description='Gender:', continuous_update=False)\n",
    "age_slider = widgets.IntSlider(value=35, min=15, max=90, step=1, description='Age (years):', continuous_update=False)\n",
    "tremble_slider = widgets.FloatSlider(value=1.0, min=0.0, max=12.0, step=0.1, description='Tremble:', continuous_update=False)\n",
    "ambitus_slider = widgets.FloatSlider(value=1.0, min=0.5, max=1.5, step=0.01, description='Ambitus:', continuous_update=False)\n",
    "pitch_slider = widgets.IntSlider(value=0, min=-12, max=12, step=1, description='Pitch (semitones):', continuous_update=False)\n",
    "\n",
    "gender_label = widgets.Label(value='Male (-1.72) → Female (1.94)')\n",
    "age_label = widgets.Label(value='Age in years')\n",
    "tremble_label = widgets.Label(value='Tremble Amount')\n",
    "ambitus_label = widgets.Label(value='Pitch Variance')\n",
    "pitch_label = widgets.Label(value='-12 to +12 semitones')\n",
    "\n",
    "process_button = widgets.Button(description='Process Audio', button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "def process_audio(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        gender = gender_slider.value\n",
    "        age_years = age_slider.value\n",
    "        age = years_to_age_param(age_years)\n",
    "        tremble = tremble_slider.value\n",
    "        ambitus = ambitus_slider.value\n",
    "        semitones = pitch_slider.value\n",
    "        pitch = semitones_to_pitch(semitones)\n",
    "        \n",
    "        print(f\"Audio file: {audio_path.value}\")\n",
    "        print(f\"Gender: {gender_param_to_label(gender)}\")\n",
    "        print(f\"Age: {age_years} years\")\n",
    "        print(f\"Tremble: {tremble}\")\n",
    "        print(f\"Ambitus: {ambitus}\")\n",
    "        print(f\"Pitch: {semitones:+d} semitones\")\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            x, sr = librosa.load(audio_path.value, sr=16000, mono=True)\n",
    "            x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            speaker_gender = torch.tensor([gender], dtype=torch.float32)\n",
    "            speaker_age = torch.tensor([age], dtype=torch.float32)\n",
    "            speaker_tremble = torch.tensor([tremble], dtype=torch.float32)\n",
    "            speaker_ambitus = torch.tensor([ambitus], dtype=torch.float32)\n",
    "            speaker_pitch = torch.tensor([pitch], dtype=torch.float32)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                vc_model.reset_pitch()\n",
    "                vc_model.set_new_speaker(speaker_gender, speaker_age)\n",
    "                vc_model.set_tremble_depth(speaker_tremble)\n",
    "                vc_model.set_ambitus_scaler(speaker_ambitus)\n",
    "                vc_model.set_pitch_mult(speaker_pitch)\n",
    "            \n",
    "            out = vc_model(x)\n",
    "            \n",
    "            display_audios([(\"INPUT\", x, sr), (\"CONVERTED\", out, sr)])\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "process_button.on_click(process_audio)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    audio_path,\n",
    "    widgets.HBox([gender_slider, gender_label]),\n",
    "    widgets.HBox([age_slider, age_label]),\n",
    "    widgets.HBox([tremble_slider, tremble_label]),\n",
    "    widgets.HBox([ambitus_slider, ambitus_label]),\n",
    "    widgets.HBox([pitch_slider, pitch_label]),\n",
    "    process_button,\n",
    "    output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29561a7b-3539-4dfc-bd81-b59ca9d7665d",
   "metadata": {},
   "source": [
    "## Convert by Audio Reference\n",
    "You can convert your input to sound like a audio reference by providing a target audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbd6ac8e-c518-45e7-8a63-22190543ea60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8495510559456aa70a2cf2fcbd120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='audio/librispeech2.wav', continuous_update=False, description='Input Audio:', place…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_audio_path = widgets.Text(\n",
    "    value='audio/librispeech2.wav',\n",
    "    placeholder='Enter input audio file path',\n",
    "    description='Input Audio:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "target_audio_path = widgets.Text(\n",
    "    value='targets/p228_004.wav',\n",
    "    placeholder='Enter target audio file path',\n",
    "    description='Target Audio:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "target_start_sample = widgets.IntText(\n",
    "    value=8000,\n",
    "    description='Target Start (in samples):',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(description='Process Audio', button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "def process_audio(b):\n",
    "    with output:\n",
    "        output.clear_output()        \n",
    "        try:\n",
    "            x, sr = librosa.load(input_audio_path.value, sr=16000, mono=True)\n",
    "            x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            t, sr = librosa.load(target_audio_path.value, sr=16000, mono=True)\n",
    "            start_idx = target_start_sample.value\n",
    "            t = torch.tensor(t[start_idx:], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            print(f\"Input audio: {input_audio_path.value}\")\n",
    "            print(f\"Target audio: {target_audio_path.value}\")\n",
    "            print(f\"Start sample: {start_idx}\")\n",
    "            print()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                vc_model.reset_pitch()\n",
    "                vc_model.set_embedding_from_audio(t)\n",
    "            \n",
    "            out = vc_model(normalize(x, transform))\n",
    "            display_audios([(\"INPUT\", x, sr), (\"TARGET\", t, sr), (\"CONVERTED\", out, sr)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "process_button.on_click(process_audio)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    input_audio_path,\n",
    "    target_audio_path,\n",
    "    target_start_sample,\n",
    "    process_button,\n",
    "    output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ff99543-2e42-4576-a373-c6b0c7cb01d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd02b1ec1b541b380b0af8599689417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='audio/librispeech2.wav', continuous_update=False, description='Input Audio:', place…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_audio_path = widgets.Text(\n",
    "    value='audio/librispeech2.wav',\n",
    "    placeholder='Enter input audio file path',\n",
    "    description='Input Audio:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "speaker_id = widgets.Text(\n",
    "    value='p231',\n",
    "    placeholder='Enter VCTK speaker ID',\n",
    "    description='Speaker ID:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "json_path = widgets.Text(\n",
    "    value='utils/speaker_dict.json',\n",
    "    placeholder='Enter JSON path',\n",
    "    description='JSON Path:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(description='Process Audio', button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "def process_audio(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        try:\n",
    "            x, sr = librosa.load(input_audio_path.value, sr=16000, mono=True)\n",
    "            x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            target = [speaker_id.value]\n",
    "            speaker_embedding_avg, speaker_embedding_one, speaker_mean = get_speaker_embeddings_json(target, json_path.value)\n",
    "            \n",
    "            print(f\"Speaker ID: {target[0]}\")\n",
    "            print(f\"F0 Mean: {speaker_mean[0]:.2f}\")\n",
    "            print()\n",
    "            \n",
    "            speaker_mean = torch.tensor([speaker_mean[0]], dtype=torch.float32)\n",
    "            speaker_embedding_avg = speaker_embedding_avg[0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                vc_model.reset_pitch()\n",
    "                vc_model.set_new_speaker_from_embedding(speaker_mean, speaker_embedding_avg)\n",
    "            \n",
    "            out = vc_model(normalize(x, transform))\n",
    "            display_audios([(\"INPUT\", x, sr), (\"CONVERTED\", out, sr)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "process_button.on_click(process_audio)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    input_audio_path,\n",
    "    speaker_id,\n",
    "    process_button,\n",
    "    output\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
